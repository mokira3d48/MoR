{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe3e049",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d91334",
   "metadata": {},
   "source": [
    "Pourquoi MoR est une petite révolution ?\n",
    "Imaginez un modèle de langage qui combine l’efficacité mémoire\n",
    "des architectures à partage de paramètres (comme ALBERT) avec l’intelligence\n",
    "computationnelle du calcul adaptatif (comme Mixture-of-Depths) :\n",
    "c’est exactement ce que propose Mixture-of-Recursions (MoR).\n",
    "Au lieu d’utiliser des couches distinctes, MoR réutilise un même bloc\n",
    "de calcul de manière récursive, tandis qu’un routeur léger décide\n",
    "dynamiquement pour chaque mot s’il doit « sortir » rapidement ou \"réfléchir\"\n",
    "plus longtemps en passant par des recursions supplémentaires. Résultat ?\n",
    "Une réduction simultanée de la taille du modèle (-50% de paramètres),\n",
    "du temps de calcul (meilleur débit d’inférence) et de la mémoire cache,\n",
    "sans sacrifier la performance --- ouvrant la voie à des LLMs à la fois agiles,\n",
    "économiques et puissants. Une avancée architecturale majeure qui mérite\n",
    "d’être explorée dans les moindres détails !\n",
    "\n",
    "Les LLMs sont puissants, mais gourmands en mémoire et en calcul.\n",
    "Et si on pouvait créer un modèle à la fois compact et intelligent,\n",
    "capable de concentrer ses efforts sur les mots qui en valent vraiment\n",
    "la peine ?\n",
    "\n",
    "C'est la réponse à cette question qui a donné naissance\n",
    "à la **Mixture-of-Recursions (MoR)**. une nouvelle architecture\n",
    "qui permet à un modèle de langage d'allouer intelligemment son « effort\n",
    "de calcul » de manière adaptive, token par token. Imaginez une usine\n",
    "de traitement où les produits simples sont expédiés rapidement\n",
    "après une étape, tandis que les produits complexes passent\n",
    "par plusieurs stations de contrôle pour un travail approfondi.\n",
    "MoR opère de la même façon : en réutilisant un même groupe de couches\n",
    "de neurones de manière recursive, et en utilisant un mécanisme de décision\n",
    "légère pour déterminer quels mots méritent plus de « réflexion ».\n",
    "Cette méthode unifie pour la première fois les gains en efficacité\n",
    "mémoire (moins de paramètres) et en efficacité computationnelle\n",
    "(moins de calculs superflus), sans compromettre les performances.\n",
    "\n",
    "Dans ce didacticiel, nous commencerons par un rappel des concepts\n",
    "fondamentaux nécessaires à la compréhension. Ensuite, nous définirons\n",
    "précisément le problème qui se pose. Puis, nous détaillerons\n",
    "le fonctionnement de la solution proposée par MoR pour résoudre le problème,\n",
    "Nous illustrerons cette solution par des exemples concrets\n",
    "et une implémentation simplifiée en Python.\n",
    "Nous discuterons ensuite des apports majeurs et des limites de cette solution,\n",
    "et proposerons des pistes d'amélioration futures,\n",
    "et enfin conclure sur la portée de ce travail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f2c34-e87b-4d58-942c-870ced804afa",
   "metadata": {},
   "source": [
    "# Connaissances de Base Nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a394f-3a1f-4df9-9b41-38261e43c409",
   "metadata": {},
   "source": [
    "Pour comprendre ce papier, il faut se familiariser avec quelques concepts\n",
    "clés des modèles de type Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6d300-8953-4a7a-be49-f88e79982de8",
   "metadata": {},
   "source": [
    "## Architecture Transformer de base (Vaswani et al., 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25023348-904f-4341-8f7f-8a5b43d9e761",
   "metadata": {},
   "source": [
    "Un transformer est un modèle de réseau de neurones qui utilise\n",
    "des **mécanismes d'attention** pour traiter des séquences de données\n",
    "(comme du texte). Il est composé de couches (layers)\n",
    "empilées les unes sur les autres. Chaque couche a une **auto-attention**\n",
    "et un réseau **feed-forward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ff125e-d625-4787-82b1-0600f5fb85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95424881-04ec-46c5-9800-e2269043af5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
