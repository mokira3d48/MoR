# Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation

**Abstract:** Scaling language models unlocks impressive capabilities, but the accompanying computational
and memory demands make both training and deployment expensive. Existing efficiency efforts typically
target either parameter sharing or adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes
of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps
to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically
assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention
computation only among tokens still active at a given recursion depth, further improving memory access
efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV
sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and
memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier:
at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-
shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These
gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.

## 1. Introduction

Scaling Transformer networks to hundreds of billions of parameters has unlocked impressive few-shot
generalization and reasoning abilities (Brown et al., 2020; Chowdhery et al., 2023; Llama Team, 2024; OpenAI,
2023; Gemini Team, 2024; DeepSeek-AI, 2024; Gemini Team, 2025). However, the accompanying memory footprint and computational requirements make both training and deployment outside hyperscale data centers
challenging (Patterson et al., 2021; Momeni et al., 2024). This has motivated researchers to seek alternative
â€œefficientâ€ designs (Tay et al., 2022; Wan et al., 2023). Among the different axes of efficiency, parameter
efficiency (Dehghani et al., 2018; Bae et al., 2024; Shazeer et al., 2017; Fedus et al., 2022; LeCun et al.,
1989)â€”reducing or sharing model weightsâ€”and adaptive computation (Raposo et al., 2024; Schuster et al.,
2022; Fedus et al., 2022; Leviathan et al., 2023)â€”spending more compute only when it is neededâ€”are
promising, actively studied research directions.

One proven route to parameter efficiency is layer tying, in which a shared set of weights is reused across
multiple layers (Dehghani et al., 2018; Lan et al., 2019; Takase and Kiyono, 2021; Gholami and Omar, 2023;
Bae et al., 2024). For adaptive computation, a common approach is early-exiting, which dynamically allocates
compute by exiting earlier in the network when predicting simpler tokens (Elhoushi et al., 2024; Schuster et al.,
2022; Elbayad et al., 2020; Bae et al., 2023). Despite the progress achieved along each of these individual
efficiency axes, an architecture that effectively unifies both parameter efficiency and adaptive computation is
still missing. Recursive Transformers (Bae et al., 2024; Fan et al., 2024; Giannou et al., 2023; Yang et al., 2023;
Saunshi et al., 2025; Geiping et al., 2025; Aleksandrov et al., 2025), models that repeatedly apply the same set
of shared layers multiple times, offer a strong foundation due to their built-in weight sharing. However, prior
attempts at dynamic recursion have often been constrained by practical hurdles, such as requiring additional
specialized training procedures or facing challenges in efficient deployment. This has led most approaches to
still default to a simpler fixed-depth recursion, which applies the same amount of computation to every token
and is thus incapable of delivering truly adaptive token-level compute allocation.

In this work, we introduce Mixture-of-Recursions (MoR), a unified framework that fully leverages the
potential of Recursive Transformers (see Figure 1). MoR trains lightweight routers end-to-end to assign
token-specific recursion depths: it decides how many times a shared parameter block is applied to each token
according to its required depth of â€œthinkingâ€, thereby directing computation to where it is most needed. This
dynamic, token-level recursion inherently facilitates recursion-wise keyâ€“value (KV) caching, selectively storing
and retrieving keyâ€“value pairs corresponding to each tokenâ€™s assigned recursion depth. This targeted caching
strategy reduces memory traffic, thereby improving throughput without relying on post-hoc modifications.
Therefore, MoR simultaneously (i) ties weights to cut parameters, (ii) routes tokens to cut redundant FLOPs,
and (iii) caches key-values recursion-wise to cut memory trafficâ€”all inside a single architecture.

Conceptually, MoR provides a pre-training framework for latent space reasoningâ€”performing non-verbal
thinking by iteratively applying a single parameter block (Hao et al., 2024; Geiping et al., 2025; Goyal et al.,
2023). However, unlike approaches that deliberate on augmented continuous prompts before generation (Liu
et al., 2024b; Goyal et al., 2023; Hao et al., 2024; Shen et al., 2025), MoR enables this latent thinking directly
during the decoding of each token (Zelikman et al., 2024). Furthermore, routing mechanism facilitates adaptive
reasoning along the modelâ€™s vertical axis1, moving beyond the uniform, fixed thinking depth common in prior
work (Geiping et al., 2025; Tack et al., 2025). In essence, MoR enables models to efficiently adjust their
thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.

**Contributions.** In summary, our key contributions in this paper are as follows.

- **Unified framework for efficient language modeling:** We present Mixture-of-Recursions (MoR), the first archi-
tecture to unify efficiency paradigmsâ€”parameter sharing (Â§2.1), token-level adaptive thinking depth (Â§2.2.1),
and memory-efficient KV caching (Â§2.2.2)â€”within a single framework.
- **Dynamic recursion routing:** We introduce a router trained from scratch to assign dynamic per-token
recursion depths. This aligns training with inference-time behavior and eliminates the need for costly,
performance-degrading post-hoc routing stages used in conventional early-exit methods.
- **Extensive empirical validation:** Across models from 135M to 1.7B parameters2 under equal compute
budgets, MoR establishes a new Pareto frontier by improving validation loss and few-shot accuracy relative
to vanilla and recursive baselines (Â§3.1, Â§3.2).
- **Efficient architecture:** MoR dramatically reduces training FLOPs by selectively engaging only essential
sequences in attention operations. Simultaneously, reduction in KV cache sizes leads to enhanced inference
throughput itself, further boosted by continuous depth-wise batching (Â§3.3).


| Layers | Cycle Strategy | | Middle-Cycle Strategy | |
| :--- | :--- | :--- | :--- | :--- |
| | **Equation** | **Figure** | **Equation** | **Figure** |
| Last | - | | $f(\mathbf{h}_{t}^{L-1}; \Phi_{L-1})$ | !(https://i.imgur.com/your-image-url-for-layer-3.png) |
| Recursion | $f(\mathbf{h}_{t}^{\ell}; \Phi_{\ell \bmod (L/N_{r})})$ | !(https://i.imgur.com/your-image-url-for-recursion.png) | $f(\mathbf{h}_{t}^{\ell}; \Phi_{(\ell-1 \bmod ((L-2)/N_{r}))+1})$ | !(https://i.imgur.com/your-image-url-for-recursion-middle.png) |
| First | - | | $f(\mathbf{h}_{t}^{0}; \Phi_{0})$ | !(https://i.imgur.com/your-image-url-for-layer-0.png) |


**Table 1:** Parameter-sharing strategies in Recursive Transformers. This table shows Cycle and Middle-Cycle schemes with
cyclic layer reuse, where Middle-Cycle retains unique first and last layers.


## 2. Method

### 2.1. Preliminary

**Recursive Transformers.** The standard Transformer (Vaswani et al., 2017) constructs token representations
through a stack of ð¿ unique layers, each with a self-attention and a feed-forward network. At time step $t$, the hidden state $h$ evolves as: $h_{t}^{l + 1} = f(h_{t}^{l}, \Phi_{l})$, 
where $l = 0, \ldots, L - 1$ and $\Phi_l$ represents the parameters of the $l$-th layer.
Recursive Transformers (Bae et al., 2024; Fan et al., 2024; Giannou et al., 2023; Yang et al.,
2023; Saunshi et al., 2025) aim to reduce parameter count by reusing layers across depth. Instead of having $L$ distinct sets of weights, they partition the model into $N_r$
recursion blocks, where each block uses a shared pool of parameters $\Phi'$. 
This design allows for more computation (by increasing the effective network depth)
without increasing parameter size.

**Parameter-sharing strategies.** We examine four parameter-sharing strategies: Cycle, 
Sequence, and their variants Middle-Cycle and Middle-Sequence.
**Table 1** summarizes two main designs, and the full list is provided in
**Table 5** in the Appendix.  In Cycle sharing, recursion blocks are reused cyclically. For 
example, consider an original non-recursive model with $L = 9$ layers and its recursive 
counterpart using $N_r = 3$ recursions. Under the â€œCycleâ€ strategy, the layers are shared 
and unrolled as ` [(0, 1, 2), (0, 1, 2), (0, 1, 2)]`.  In â€œSequenceâ€ sharing, each recursion
block reuses the same layer consecutively before moving to the next,
resulting in  `[(0, 0, 0), (1, 1, 1), (2, 2, 2)]` for the same configuration.
Both have the same effective number of layers when unrolled ($L = 9$), 
but with a different order. Furthermore, the â€œMiddleâ€ variants preserve full-capacity 
parameters at the first and last layers ($\Phi_0$ and $\Phi_{L - 1}$),
while sharing weights among the intermediate layers.

**Enhanced training and inference efficiency in recursive models.** Parameter sharing strategies can reduce
the number of unique trainable parameters by a factor of the recursion number, effectively amortizing the
memory footprint of the model. From a distributed training perspective, this becomes highly efficient when
using Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023). While a single `all-gather ` 
operation would only
support one iteration previously (i.e., 1 iter/gather), a recursive model reuses the same gathered parameters
across all recursive steps (i.e., $N_r$ iter/gather). 
Furthermore, recursive architectures enable a novel inference
paradigm, continuous depth-wise batching (Bae et al., 2024; Hooper et al., 2023). This technique allows tokens
at different stages to be grouped into a single batch, as they all utilize the same block of parameters. This can
eliminate the bubblesâ€”idle periods spent waiting for other samples to completeâ€”thereby leading to significant
throughput gains.

**Limitations in prior works.** Although model parameters are tied, the distinct KV caches are typically used
for each depth. This design fails to reduce the cache sizes, meaning the high retrieval latency still remains a
severe inference bottleneck. Moreover, most existing recursive models simply apply a fixed recursion depth to
all tokens, ignoring the varying complexity. While post-hoc methods like early-exiting methods can introduce
some adaptivity, they often require separate training phases that can degrade performance (Schuster et al.,
2022; Elhoushi et al., 2024; Bae et al., 2024). Ideally, the recursion depth should be learned dynamically during
pretraining, allowing the model to adapt its computational path to each tokenâ€™s difficulty in a data-driven
manner. However, such dynamic paths introduce a new challenge: exited tokens will have missing KV pairs at
subsequent recursion depths. Addressing this would require a parallel decoding mechanism (Bae et al., 2023;
Elhoushi et al., 2024; Kim et al., 2023b) to efficiently compute the actual KV pairs, but this requires separate,
complex engineering and complicates the system.

![]()

**Figure 2:** Architectural components of Mixture-of-Recursions (MoR). (a) Expert-choice routing: At each recursion step,
a router selects top-ð‘˜ tokens to continue, progressively narrowing the set of active tokens with depth. (b) Token-choice
routing: Each token is assigned a fixed recursion step at the outset via a single routing decision, defining its complete
compute path through the model. (c) KV caching strategies: Each square in the matrix represents whether a token (row)
attends to another tokenâ€™s cached key (column). In â€œrecursion-wise KV cachingâ€ (Top), only the keys of currently selected
(non-dropped) tokens at each recursion step are cached ( blue ), and attention is restricted only to these entries. In
â€œrecursive KV sharingâ€ (Bottom), all keys of previous tokens are cached at the first recursion step ( purple ) and shared
across subsequent recursion steps for attention operations.

## 2.2. Mixture-of-Recursions
We propose Mixture-of-Recursions (MoR)â€”a framework that dynamically adjusts recursion step for each token
during pretraining and inference. The core of MoR lies in two components: a routing mechanism that assigns
token-specific recursion steps to adaptively concentrate computation on more challenging tokens, and a KV
caching strategy that defines how KV pairs are stored and selectively utilized for attention at each recursive step.


### 2.2.1. Routing Strategies: Expert-choice vs. Token-choice

**Expert-choice routing. (Figure 2a)** Inspired by top-ð‘˜ gating in MoD models (Raposo et al., 2024), in expert-
choice routing, each recursion depth becomes an expert and selects their preferred top-$ð‘˜$ tokens (e.g., for $N_r = 3$ we have three experts: Expert 1 applies the first recursion step, Expert 2 applies the second recursion step, and
so on) . At each recursion step $r$, the corresponding router uses the hidden state
$\mathcal{H}_{t}^{r}$ (input to the $r$-th recursion
block) and its routing parameters $\theta_r$ to compute a scalar score 
$ g_{t}^{r} = \mathcal{G}(\theta_{t}^{T} \mathcal{H}_{t}^{r})$ for token $t$. Here, $\mathcal{G}$
represents an
activation function like `sigmoid` or `tanh`. Then, the top-$ð‘˜$ tokens are selected to pass through the recursion block: 

$$
\mathcal{H}_{t}^{r+1} =
\begin{cases}
g_{t}^{r} f(\mathcal{H}_{t}^{r}, \Phi') + \mathcal{H}_{t}^{r}, & \text{if } g_{t}^{r} > P_{\beta}(G^{r}) \\
\mathcal{H}_{t}^{r}, & \text{otherwise}
\end{cases}
$$

where $P_{\beta}(G^{r})$ is the $\beta$-percentile threshold over all scores at recursion step $r$.

To ensure coherent progression through steps, we adopt hierarchical filtering: only tokens selected at
recursion step $r$  can be re-evaluated at $r + 1$ This simulates early-exit behavior while learning from scratch. As
deeper layers tend to encode increasingly abstract and sparse information (Li et al., 2022; Yang et al., 2024;
Nawrot et al., 2024), this mechanism prioritizes computation for only the most demanding tokens.

**Token-choice routing. (Figure 2b)** Unlike expert-choice, where token selection is made at each recursion
step, token-choice commits each token to a full sequence of recursion blocks from the start. Formally, given
the hidden state $\mathcal{H}_{t}^{1}$ (in Middle-Cycle strategy, $\mathcal{H}_{t}^{1} = h_{t}^{1}$), the router computes a non-linear function (`softmax` or `sigmoid`) over experts:
$g_t = \mathcal{G}(\theta_{r}^{T} \mathcal{H}_{t}^{1})$, where $g_{t}^{j}$ denotes
 the routing score for expert $j \in \{1, \ldots, N_r\}$. The
token is assigned to expert $i = \arg \max_{j} g_{t}^{j}$ (top-1 gating), which corresponds 
to sequentially applying the recursion $i$ times. The hidden state is then updated recursively as:

$$
\mathcal{H}_{t}^{r+1} =
\begin{cases}
g_{t}^{r}f(\mathcal{H}_{t}^{r}, \Phi') + \mathcal{H}_{t}^{1}, & \text{if } r = i \\
g_{t}^{r}f(\mathcal{H}_{t}^{r}, \Phi'), & \text{otherwise}
\end{cases}
$$


| | **Expert-choice** | **Token-choice** | | **Recursion-wise Caching** | **Recursive Sharing** |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Pros** | Static compute budget | No leakage | **KV Memory** | $(N_r + 1)/2N_r$ | $1/N_r$ |
| **Cons** | Causality violation | Load imbalance | **KV Cache IO** | $(N_r + 1)/2N_r$ | 1 |
| **Sol.** | Aux Rout, Aux Loss | Bal Loss, Loss-free | **Attn FLOPs** | $k^2 / N_{ctx}$ | $k/N_{ctx}$ |

**Table 2:** Comparison of routing strategies and key-value caching strategies. (Left) Summary of two routing strategies:
expert-choice and token-choice, highlighting their pros, cons, and mitigating solutions from previous works (Raposo et al.,
2024; Wang et al., 2024; Zoph et al., 2022). (Right) Relative cost efficiency of caching strategies against a vanilla Transformer
(normalized to 1). Here, $N_r$ denotes the number of recursions, and $k$ ($k < N_{ctx}$) 
denotes the number of selected tokens
per layer. KV cache memory and IO are measured across the entire model, whereas attention FLOPs are reported per layer.


To compare routing strategies under equal compute, we align the token allocation budgets of expert-choice
with that of token-choice. Specifically, we calibrate token capacity (i.e., top-$k$) of expert-choice to match
the expected token distribution of token-choice routing with perfect load balancing. In perfectly balanced
token-choice, each token is assigned to recursion depth $i \in \{1, \ldots, N_r\}
with equal probability $1/N_r$. Thus, recursion step $j$ processes a fraction
$(N_r - j + 1)/N_r$ of the tokens. For example, when $N_r = 3$, 
recursion steps $1$, $2$, and $3$ handle $\{3/3, 2/3, 1/3\}$ of tokens, respectively. Therefore, we apply this same fractional allocation in the top-$ð‘˜$ selection
of the expert-choice routing (i.e., $k$ is sequenced like $N_r/N_r, \ldots, 1/N_r$
over $N_r$ recursion steps).

**Strengths and limitations. (Table 2-Left)** Although expert-choice routing guarantees perfect load balancing with static **top-$k$ selection**, it suffers from information leakage (Zhou et al., 2022; Wang et al., 2024; Raposo et al., 2024). This violation of causality during training forces to exploit an auxiliary router or a regularization loss (Zhou et al., 2022; Raposo et al., 2024), aiming to precisely detect top-$k$ tokens at inference without access to future token information. Meanwhile, **token-choice** is free from such leakage, but typically requires a balancing loss or loss-free algorithms (Wang et al., 2024; Fedus et al., 2022; Zoph et al., 2022) due to its inherent load balancing challenges. We explore each of these components for MoR in further detail (Â§4.2).


### 2.2.2. KV Caching Strategies: Recursion-wise Caching vs. Recursive Sharing

Dynamic-depth models often struggle with KV cache consistency during autoregressive decoding. When a token exits early, its corresponding keys and values in deeper layers will be missing, which can be crucial information for subsequent tokens. Some methods attempt to reuse stale entries (Schuster et al., 2022) or run parallel decoding (Bae et al., 2023), but these solutions still introduce overhead and complexity. To this end, we design and explore two KV cache strategies tailored to MoR models: **recursion-wise caching** and **recursive sharing**.


**Recursion-wise KV caching. (Figure 2c-Top)**
Inspired by Raposo et al. (2024), we cache KV pairs selectively: only tokens routed to a given recursion depth step store their key-value entries at that level. Thereby, the KV cache size at each recursion depth is determined exactly by the capacity factor in expert-choice, or according to actual balancing ratios in token-choice. Attention is then restricted to those locally cached tokens. This design promotes block-local computation, which improves memory efficiency and reduces IO demands.

**Recursive KV sharing. (Figure 2c-Bottom)**
A key design choice for our MoR model is that all tokens traverse at least the first recursion block. We leverage this by caching KV pairs exclusively at this initial step and reusing them across all subsequent recursions. Therefore, the query length might get shorter at each recursion depth based on the selection capacity, but the key and value lengths will consistently maintain the full sequence. This ensures that all tokens can access to past context without recomputation, despite any distribution mismatch.


**Strengths and limitations. (Table 2-Right)**
Recursion-wise caching cuts KV memory and IO to approximately $(N_r + 1)/2N_r$ times across the entire model (when assuming capacity factors follow a sequence like $N_r/N_r, \dots, 1/N_r$ over $N_r$ recursion steps). It also reduces per-layer attention FLOPs to a factor of $(k/N_{ctx})^2$ of those in vanilla models, resulting in substantially improved efficiency for both training and inference phases. Meanwhile, recursive sharing can yield maximal memory savings by globally reusing context. Specifically, significant speedups can be achieved by skipping KV projection and prefill operations at shared depths (Sun et al., 2024). However, attention FLOPs only decrease by a factor of $k/N_{ctx}$, and high volume of KV IO still leads to a decoding bottleneck.


| Models | MoR | MoR | Recursion | | | | Pretrain | | | NLL$\downarrow$ | | | Few-shot Accuracy$\uparrow$ | | | | |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| | Type | KV | Share | $N_r$ | Param | FLOPs | $N_{tok}$ | FineWeb | LD | HS | PQ | WG | ARC | MMLU | Avg |
| Vanilla | - | - | - | - | 315M | 16.5 | 20B | 2.7824 | 32.0 | 37.8 | 65.6 | 50.5 | 39.6 | 28.0 | 42.3 |
| Recursive | - | - | M-Cyc | 2 | 167M | 16.5 | 20B | 2.8079 | 31.0 | 37.1 | 66.7 | 52.3 | **40.8** | 27.5 | 42.6 |
| | - | - | M-Cyc | 3 | 118M | 16.5 | 20B | 2.8466 | 29.8 | 35.9 | 65.0 | 52.3 | 39.0 | 27.2 | 41.5 |
| | - | - | M-Cyc | 4 | 98M | 16.5 | 19B | 2.8781 | 28.2 | 35.4 | 65.5 | 52.5 | 38.0 | 26.8 | 41.0 |
| MoR (ours) | Expert | Cache | M-Cyc | 2 | 167M | 16.5 | 27B | **2.7511** | **34.4** | **39.3** | 65.7 | 51.2 | 39.6 | **28.1** | **43.1** |
| | Expert | Cache | M-Cyc | 3 | 118M | 16.5 | 30B | 2.7925 | 33.1 | 37.9 | **66.9** | 52.1 | 38.3 | 27.4 | 42.6 |
| | Expert | Cache | M-Cyc | 4 | 98M | 16.5 | 30B | 2.8204 | 30.1 | 37.3 | 65.0 | 51.1 | 38.9 | 27.4 | 41.6 |
| | Expert | Cache | M-Cyc | 2 | 167M | 12.3 | 20B | 2.7749 | 33.2 | 38.3 | 65.2 | **52.6** | 40.1 | **28.1** | 42.9 |
| | Expert | Cache | M-Cyc | 3 | 118M | 11.0 | 20B | 2.8246 | 31.9 | 37.0 | 65.7 | 50.5 | 38.3 | 27.4 | 41.8 |
| | Expert | Cache | M-Cyc | 4 | 98M | 11.0 | 20B | 2.8519 | 30.2 | 36.5 | 64.3 | 52.3 | 38.6 | 27.2 | 41.5 |
| | Token | Cache | M-Cyc | 3 | 118M | 16.5 | 30B | 2.9163 | 27.6 | 34.1 | 63.8 | 50.6 | 37.4 | 26.8 | 40.0 |
| | Expert | Share | M-Cyc | 3 | 118M | 16.5 | 31B | 2.7983 | 31.7 | 37.2 | 65.1 | 51.0 | 39.0 | 27.1 | 41.9 |

**Table 3:** Comparison of MoR, Recursive, and Vanilla Transformers under both fixed FLOPs (16.5e18) and token (20B) settings. All models are trained on FineWeb-Edu and evaluated by validation negative log-likelihood (NLL) and few-shot accuracy. For the isoFLOP rows, the number of training tokens ($N_{tok}$) varies by model efficiency. For the fixed-token rows, we report the effective FLOPs consumed. For the model sizes, we report non-embedding parameter counts. For the KV mechanisms, we distinguish between Cache (recursion-wise caching) and Share (recursive sharing). In recursive models, all tokens go through fixed recursion depths ($N_r$), instead of adaptive depths.
